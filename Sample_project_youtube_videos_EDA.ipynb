{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc88f6c2-0fe6-4809-bd30-668aff01e30c",
   "metadata": {},
   "source": [
    "# Youtube LLM Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21458ddf-101b-4ecd-8ee8-3cf6655eb46b",
   "metadata": {},
   "source": [
    "## 1.4. Dataset\n",
    "\n",
    "### Data selection\n",
    "\n",
    "As this project is particularly focused on data science channels, I found that not many readily available datasets online are suitable for this purpose. The 2 alternative datasets I found are:\n",
    "\n",
    "- [The top trending Youtube videos on Kaggle](https://www.kaggle.com/rsrishav/youtube-trending-video-dataset): This dataset contains several months of data on daily trending YouTube videos for several countries. There are up to 200 trending videos per day. However, this dataset is not fit for this project because the trending videos are about a wide range of topics that are not necessarily related to data science. \n",
    "\n",
    "- Another dataset is obtained from this [Github repo](https://gitlab.com/thebrahminator/Youtube-View-Predictor) of Vishwanath Seshagiri, which is the metadata of 0.5M+ YouTube videos along with their channel data. There is no clear documentation on how this dataset was created, but a quick look at the datasets in the repository suggested that the data was obtained using keyword search of popular keywords such as \"football\" or \"science\". There are also some relevant keywords such as \"python\". However, I decided not to use these datasets because they don't contain data for the channels I am interested in.\n",
    "\n",
    "I created my own dataset using the [Google Youtube Data API version 3.0](https://developers.google.com/youtube/v3). The exact steps of data creation is presented in section *2. Data Creation* below.\n",
    "\n",
    "### Data limitations\n",
    "\n",
    "The dataset is a real-world dataset and suitable for the research. However, the selection of the top 10 Youtube channels to include in the research is purely based on my knowledge of the channels in data science field and might not be accurate. My definition is \"popular\" is only based on subscriber count but there are other metrics that could be taken into consideration as well (e.g. views, engagement). The top 10 also seems arbitrary given the plethora of channels on Youtube. There might be smaller channels that might also very interesting to look into, which could be the next step of this project.\n",
    "\n",
    "### Ethics of data source\n",
    "\n",
    "According to [Youtube API's guide](https://developers.google.com/youtube/v3/getting-started), the usage of Youtube API is free of charge given that your application send requests within a quota limit. \"The YouTube Data API uses a quota to ensure that developers use the service as intended and do not create applications that unfairly reduce service quality or limit access for others. \" The default quota allocation for each application is 10,000 units per day, and you could request additional quota by completing a form to YouTube API Services if you reach the quota limit.\n",
    "\n",
    "Since all data requested from Youtube API is public data (which everyone on the Internet can see on Youtube), there is no particular privacy issues as far as I am concerned. In addition, the data is obtained only for research purposes in this case and not for any commercial interests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2ba6bd0-2c70-42d5-ab1d-9ebff01c62ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\furni\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.0' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "c:\\Users\\furni\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:62: UserWarning: Pandas requires version '1.3.4' or newer of 'bottleneck' (version '1.3.2' currently installed).\n",
      "  from pandas.core import (\n",
      "c:\\Users\\furni\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dateutil import parser\n",
    "import isodate\n",
    "from datetime import datetime, timedelta\n",
    "from googleapiclient.errors import HttpError \n",
    "\n",
    "# Data visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\", color_codes=True)\n",
    "\n",
    "# Google API\n",
    "from googleapiclient.discovery import build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab8a223f-2420-421f-8647-344591a19e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\furni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\furni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# NLP libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be282b2-b566-4587-97d1-71b680cf6b92",
   "metadata": {},
   "source": [
    "# 2. Data creation with Youtube API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400f8704-cfe4-495b-af35-bb5dfb677c6e",
   "metadata": {},
   "source": [
    "I first created a project on Google Developers Console, then requested an authorization credential (API key). Afterwards, I enabled Youtube API for my application, so that I can send API requests to Youtube API services. Then, I went on Youtube and checked the channel ID of each of the channels that I would like to include in my research scope (using their URLs). Then I created the functions for getting the channel statistics via the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "359fc781",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = 'AIzaSyB-4NIQtecQPbRX7TWKphThkb9_Brh2wL4' \n",
    "#api_key = 'AIzaSyA4Sd1FkOSah19dL7cg7OuBUj9VBJiE2fE'\n",
    "\n",
    "# channel_ids = ['UCtYLUTtgS3k1Fg4y5tAhLbw', # Statquest \n",
    "#                'UCCezIgC97PvUuR4_gbFUs5g', # Corey Schafer\n",
    "#                'UCfzlCWGWYyIQ0aLC5w48gBQ', # Sentdex\n",
    "#                'UCNU_lfiiWBdtULKOw6X0Dig', # Krish Naik\n",
    "#                'UCzL_0nIe8B4-7ShhVPfJkgw', # DatascienceDoJo\n",
    "#                'UCLLw7jmFsvfIVaUFsLs8mlQ', # Luke Barousse \n",
    "#                'UCiT9RITQ9PW6BhXK0y2jaeg', # Ken Jee\n",
    "#                'UC7cs8q-gJRlGwj4A8OmCmXg', # Alex the analyst\n",
    "#                'UC2UXDak6o7rBm23k3Vv5dww', # Tina Huang\n",
    "#               ]\n",
    "\n",
    "channel_ids = [\n",
    "    'UCupvZG-5ko_eiXAupbDfxWw',  # CNN\n",
    "     'UCXIJgqnII2ZOINSWNOGFThA',  # FOX NEWS\n",
    "     'UCaXkIU1QidjPwiAYu6GcHjg',  # MSNBC\n",
    "     'UCBi2mrWuNuyYy4gbM6fU18Q',  # ABC NEWS\n",
    "     'UC8p1vwvWtl6T73JiExfWs1g',  # CBS NEWS\n",
    "]\n",
    "\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e528263d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_stats(youtube, channel_ids):\n",
    "    \"\"\"\n",
    "    Get channel statistics: title, subscriber count, view count, video count, upload playlist\n",
    "    Params:\n",
    "    \n",
    "    youtube: the build object from googleapiclient.discovery\n",
    "    channels_ids: list of channel IDs\n",
    "    \n",
    "    Returns:\n",
    "    Dataframe containing the channel statistics for all channels in the provided list: title, subscriber count, view count, video count, upload playlist\n",
    "    \n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    request = youtube.channels().list(\n",
    "                part='snippet,contentDetails,statistics',\n",
    "                id=','.join(channel_ids))\n",
    "    response = request.execute() \n",
    "    \n",
    "    for i in range(len(response['items'])):\n",
    "        data = dict(channelName = response['items'][i]['snippet']['title'],\n",
    "                    channel_id=channel_ids[i],\n",
    "                    subscribers = response['items'][i]['statistics']['subscriberCount'],\n",
    "                    views = response['items'][i]['statistics']['viewCount'],\n",
    "                    totalVideos = response['items'][i]['statistics']['videoCount'],\n",
    "                    playlistId = response['items'][i]['contentDetails']['relatedPlaylists']['uploads'])\n",
    "        all_data.append(data)\n",
    "    \n",
    "    return pd.DataFrame(all_data)\n",
    "\n",
    "def get_video_ids(youtube, playlist_id):\n",
    "    \"\"\"\n",
    "    Get list of video IDs of all videos in the given playlist for the last month past(30 days) \n",
    "    Params:\n",
    "    \n",
    "    youtube: the build object from googleapiclient.discovery\n",
    "    playlist_id: playlist ID of the channel\n",
    "    \n",
    "    Returns:\n",
    "    List of video IDs of all videos in the playlist\n",
    "    \n",
    "    \"\"\"\n",
    "    one_month_ago = (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "    \n",
    "    request = youtube.playlistItems().list(\n",
    "                part='contentDetails',\n",
    "                playlistId=playlist_id,\n",
    "                maxResults=50)\n",
    "    response = request.execute()\n",
    "    \n",
    "    video_ids = []\n",
    "    \n",
    "    for i in range(len(response['items'])):\n",
    "        video_published_at = response['items'][i]['contentDetails']['videoPublishedAt']\n",
    "        \n",
    "        # Check if the video was published in the past month\n",
    "        if video_published_at >= one_month_ago:\n",
    "            video_ids.append(response['items'][i]['contentDetails']['videoId'])\n",
    "    \n",
    "\n",
    "    next_page_token = response.get('nextPageToken')\n",
    "    more_pages = True\n",
    "    \n",
    "    while more_pages:\n",
    "        if next_page_token is None:\n",
    "            more_pages = False\n",
    "        else:\n",
    "            request = youtube.playlistItems().list(\n",
    "                        part='contentDetails',\n",
    "                        playlistId = playlist_id,\n",
    "                        maxResults = 50,\n",
    "                        pageToken = next_page_token\n",
    "                        )\n",
    "            response = request.execute()\n",
    "    \n",
    "            for i in range(len(response['items'])):\n",
    "                video_published_at = response['items'][i]['contentDetails']['videoPublishedAt']\n",
    "        \n",
    "                # Check if the video was published in the past month\n",
    "                if video_published_at >= one_month_ago:\n",
    "                    video_ids.append(response['items'][i]['contentDetails']['videoId'])\n",
    "            \n",
    "            next_page_token = response.get('nextPageToken')\n",
    "    return video_ids\n",
    "\n",
    "\n",
    "\n",
    "def get_video_details(youtube, video_ids, channel_id):\n",
    "    \"\"\"\n",
    "    Get video statistics of all videos with given IDs\n",
    "    Params:\n",
    "    \n",
    "    youtube: the build object from googleapiclient.discovery\n",
    "    video_ids: list of video IDs\n",
    "    channel_id: ID of the channel\n",
    "    \n",
    "    Returns:\n",
    "    Dataframe with statistics of videos, i.e.:\n",
    "        'channel_id', 'channelTitle', 'title', 'description', 'tags', 'publishedAt'\n",
    "        'viewCount', 'likeCount', 'favoriteCount', 'commentCount'\n",
    "        'duration', 'definition', 'caption'\n",
    "    \"\"\"\n",
    "        \n",
    "    all_video_info = []\n",
    "    \n",
    "    for i in range(0, len(video_ids), 50):\n",
    "        request = youtube.videos().list(\n",
    "            part=\"snippet,contentDetails,statistics\",\n",
    "            id=','.join(video_ids[i:i+50])\n",
    "        )\n",
    "        response = request.execute() \n",
    "\n",
    "        for video in response['items']:\n",
    "            stats_to_keep = {'snippet': ['channelTitle', 'title', 'description', 'tags', 'publishedAt','defaultAudioLanguage'],\n",
    "                             'statistics': ['viewCount', 'likeCount', 'favouriteCount', 'commentCount'],\n",
    "                             'contentDetails': ['duration', 'definition', 'caption']\n",
    "                            }\n",
    "            video_info = {}\n",
    "            video_info['channel_id'] = channel_id  # Add channel_id to the DataFrame\n",
    "            video_info['video_id'] = video['id']\n",
    "\n",
    "            for k in stats_to_keep.keys():\n",
    "                for v in stats_to_keep[k]:\n",
    "                    try:\n",
    "                        video_info[v] = video[k][v]\n",
    "                    except:\n",
    "                        video_info[v] = None\n",
    "\n",
    "            all_video_info.append(video_info)\n",
    "            \n",
    "    return pd.DataFrame(all_video_info)\n",
    "\n",
    "\n",
    "\n",
    "def get_playlists_info(youtube, channel_ids):\n",
    "\n",
    "\n",
    "    all_playlist_data = []\n",
    "\n",
    "    \"\"\"\n",
    "    Retreiving Playlist data for all the channels\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    for channel_id in channel_ids:\n",
    "        request = youtube.playlists().list(\n",
    "            part=\"snippet\",\n",
    "            channelId=channel_id,\n",
    "            maxResults=50  # Adjust the maximum number of playlists to retrieve if needed\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        for playlist in response.get(\"items\", []):\n",
    "            playlist_data = dict(\n",
    "                playlist_id=playlist[\"id\"],\n",
    "                title=playlist[\"snippet\"][\"title\"],\n",
    "                description=playlist[\"snippet\"][\"description\"],\n",
    "                publishedAt=playlist[\"snippet\"][\"publishedAt\"],\n",
    "                channelId=playlist[\"snippet\"][\"channelId\"],\n",
    "                channelTitle=playlist[\"snippet\"][\"channelTitle\"],\n",
    "                defaultLanguage=playlist[\"snippet\"].get(\"defaultLanguage\"),\n",
    "                thumbnailUrl=playlist[\"snippet\"][\"thumbnails\"][\"default\"][\"url\"]\n",
    "            )\n",
    "            all_playlist_data.append(playlist_data)\n",
    "    return pd.DataFrame(all_playlist_data)\n",
    "\n",
    "\n",
    "def get_captions(youtube, video_ids):\n",
    "    caption_list = []\n",
    "\n",
    "    for video_i in video_ids:\n",
    "            captions = youtube.captions().list(\n",
    "            part=\"snippet\",\n",
    "            videoId=video_i\n",
    "        ).execute()\n",
    "\n",
    "        # List to store comments as dictionaries\n",
    "            \n",
    "\n",
    "        # Extract comments and append them to the list\n",
    "            for caption in captions[\"items\"]:\n",
    "                snippet = caption[\"snippet\"]\n",
    "                caption_dict = {\n",
    "        \"videoId\": snippet[\"videoId\"],\n",
    "        \"lastUpdated\": snippet[\"lastUpdated\"],\n",
    "        \"trackKind\": snippet[\"trackKind\"],\n",
    "        \"language\": snippet[\"language\"],\n",
    "        \"name\": snippet[\"name\"],\n",
    "        \"audioTrackType\": snippet[\"audioTrackType\"],\n",
    "        \"status\": snippet[\"status\"]\n",
    "    }\n",
    "                caption_list.append(caption_dict)\n",
    "    return(pd.DataFrame(caption_list))\n",
    "\n",
    "def get_comments(youtube, video_ids):\n",
    "    \"\"\"\n",
    "    Get top level comments as text from all videos with given IDs (only the first 50 comments per video due to quote limit of Youtube API)\n",
    "    Params:\n",
    "    \n",
    "    youtube: the build object from googleapiclient.discovery\n",
    "    video_ids: list of video IDs\n",
    "    \n",
    "    Returns:\n",
    "    Dataframe with video IDs and associated top level comment in text.\n",
    "    \n",
    "    \"\"\"\n",
    "    all_comments = []\n",
    "    all_comments_data = []    \n",
    "    for video_id in video_ids:\n",
    "        comments_in_video_info = {}\n",
    "        try:   \n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet,replies\",\n",
    "                videoId=video_id\n",
    "            )\n",
    "            response = request.execute()\n",
    "            \n",
    "            comments_in_video= []\n",
    "            comments_in_video_info = {}\n",
    "            for comment in response['items'][:50]:\n",
    "                comment_text = comment['snippet']['topLevelComment']['snippet']['textOriginal']\n",
    "        \n",
    "                # Append the comment text to the list\n",
    "                comments_in_video.append(comment_text)\n",
    "                comments_data = {'video_id': video_id, \n",
    "                                'comments': comment_text,\n",
    "                                'likeCount': comment['snippet']['topLevelComment']['snippet']['likeCount'],\n",
    "                                'authorDisplayName': comment['snippet']['topLevelComment']['snippet']['authorDisplayName'],\n",
    "                                'authorProfileImageUrl': comment['snippet']['topLevelComment']['snippet']['authorProfileImageUrl'],\n",
    "                                'authorChannelUrl': comment['snippet']['topLevelComment']['snippet']['authorChannelUrl'],\n",
    "                                'authorChannelId': comment['snippet']['topLevelComment']['snippet']['authorChannelId']['value'],\n",
    "                                'channelId': comment['snippet']['topLevelComment']['snippet']['channelId'],\n",
    "                                'canRate': comment['snippet']['topLevelComment']['snippet']['canRate'],\n",
    "                                'viewerRating': comment['snippet']['topLevelComment']['snippet']['viewerRating'],\n",
    "                                'publishedAt': comment['snippet']['topLevelComment']['snippet']['publishedAt']\n",
    "                                \n",
    "                                                            \n",
    "                                }\n",
    "                all_comments_data.append(comments_data)\n",
    "            comments_in_video_info = {'video_id': video_id, 'comments': comments_in_video}\n",
    "            \n",
    "\n",
    "\n",
    "        except: \n",
    "            # When error occurs - most likely because comments are disabled on a video\n",
    "            print('Could not get comments for video ' + video_id)\n",
    "\n",
    "\n",
    "\n",
    "        all_comments.append(comments_in_video_info)\n",
    "\n",
    "            \n",
    "                \n",
    "        \n",
    "        # Create a dictionary for each comment and append it to the list\n",
    "                # comment_info = {'video_id': video_id, 'comment': comment_text}\n",
    "                # comments_in_video_info.append(comment_info)\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "    return pd.DataFrame(all_comments_data) , pd.DataFrame(all_comments)   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507bec49",
   "metadata": {},
   "source": [
    "## CREATING DATAFRAMES FOR ALL THE TABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa92567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(api_key, video_ids, channel_id):\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "    \n",
    "\n",
    "    # Get video data for the current chunk of video IDs\n",
    "    video_data = get_video_details(youtube, video_ids, channel_id)\n",
    "\n",
    "    # Get comment data for the current chunk of video IDs\n",
    "    comments_data_df, comments_combined_df = get_comments(youtube, video_ids)\n",
    "\n",
    "    return video_data, comments_data_df, comments_combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9d27fb8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting video information from channel: Fox News\n",
      "Could not get comments for video 7QjvVfeR9r8\n",
      "Could not get comments for video oZ_vbeMSS98\n",
      "Could not get comments for video gUBWVk6QurE\n",
      "Could not get comments for video f4F0AzBb56o\n",
      "Could not get comments for video FlKA54ZUIbY\n",
      "Could not get comments for video ItesQPyCdlI\n",
      "Could not get comments for video _q9e_q93vkQ\n",
      "Could not get comments for video MXtTWFhuYbA\n",
      "Could not get comments for video m2-S-5TM0pw\n",
      "Could not get comments for video 48YaHEBPXQc\n",
      "Could not get comments for video fUY7IL9htKo\n",
      "Could not get comments for video zbWfFpeRHoA\n",
      "Could not get comments for video XICCOVgNgDA\n",
      "Could not get comments for video 70-sefxS68c\n",
      "Could not get comments for video GxeUjO23vgg\n",
      "Could not get comments for video BNQADZacua8\n",
      "Could not get comments for video ZJNLcoNvSHg\n",
      "Could not get comments for video OF-EQlrR5Ro\n",
      "Could not get comments for video 4hYkgGMpe2g\n",
      "Could not get comments for video 5KMx6azI6tE\n",
      "Could not get comments for video 4KxUZHzU-Zs\n",
      "Could not get comments for video aaBqNG8fkWo\n",
      "Could not get comments for video 65zCSZK6p1U\n",
      "Could not get comments for video Z3wTh51-bz8\n",
      "Could not get comments for video l9r7nSCPKqk\n",
      "Could not get comments for video NT0ZIOKfEL4\n",
      "Could not get comments for video NlzqxuWcZMY\n",
      "Could not get comments for video IjAuTPESJ7o\n",
      "Could not get comments for video 2qZALSfkAFA\n",
      "Could not get comments for video vqvtGMeh41g\n",
      "Could not get comments for video OOKkDcwzlAE\n",
      "Could not get comments for video gKjFcCydeGs\n",
      "Could not get comments for video VE8jwAuG9BQ\n",
      "Could not get comments for video atsL8ovYg8A\n",
      "Could not get comments for video p8YyE1O12Co\n",
      "Could not get comments for video 8Dvnb4hofaE\n",
      "Could not get comments for video FgbeHr_hybU\n",
      "Could not get comments for video jaqoKinbRxs\n",
      "Could not get comments for video 2zfOwwkiFew\n",
      "Could not get comments for video zZHwlQqDorA\n",
      "Could not get comments for video e_SE_OnfM4A\n",
      "Could not get comments for video ZDxpjOaYtiQ\n",
      "Could not get comments for video VRrzx-iI-RQ\n",
      "Could not get comments for video g4Lq0xe1Cco\n",
      "Could not get comments for video UiA7ukaZiN4\n",
      "Could not get comments for video v0ApzMawzL0\n",
      "Getting video information from channel: CNN\n",
      "Getting video information from channel: ABC News\n",
      "Could not get comments for video zbnWPhPHTGE\n",
      "Could not get comments for video UQWBOkTeYf8\n",
      "Could not get comments for video jCotIotsnVo\n",
      "Could not get comments for video Xf17NM38fn0\n",
      "Could not get comments for video ozECIlqpKJs\n",
      "Could not get comments for video uKupoNutUSA\n",
      "Could not get comments for video 6NQslhh4WiE\n",
      "Could not get comments for video C92SGYjblxI\n",
      "Could not get comments for video 3yA6B9qPNQA\n",
      "Could not get comments for video psHHvbD6W5E\n",
      "Could not get comments for video BqXBePgCBU8\n",
      "Could not get comments for video P6V0_3Ckpzo\n",
      "Could not get comments for video 7v6p8Vil-Z8\n",
      "Could not get comments for video diV4hF930Eg\n",
      "Could not get comments for video -KVcdHNOa-M\n",
      "Could not get comments for video RlGa0JRlUK4\n",
      "Could not get comments for video mRxnzdQic14\n",
      "Could not get comments for video KabehDEW-Jw\n",
      "Could not get comments for video 8sbsyFXG2sc\n",
      "Could not get comments for video 3ACP0-wavLM\n",
      "Could not get comments for video Rcv5Lj4uC30\n",
      "Could not get comments for video ZVHKO5I2hbo\n",
      "Could not get comments for video 4LYW2wuFxRw\n",
      "Could not get comments for video aUuYgWKjDPA\n",
      "Could not get comments for video MCCY4OC2uVM\n",
      "Could not get comments for video pBsN5tajMcY\n",
      "Could not get comments for video tmmTd24QAZ4\n",
      "Could not get comments for video TPN3D0v8b-U\n",
      "Could not get comments for video qxCKFk8K9aw\n",
      "Could not get comments for video xjjUQzkFUjM\n",
      "Could not get comments for video kVzPAzP2Ey0\n",
      "Could not get comments for video WkTG_15eJfs\n",
      "Could not get comments for video LivW31s3CB8\n",
      "Could not get comments for video j42n83LSu3A\n",
      "Could not get comments for video FfEkzn3tNGk\n",
      "Could not get comments for video cqTB5Mbyg48\n",
      "Could not get comments for video 8Qnepjda4g4\n",
      "Could not get comments for video q6bTXKugwy8\n",
      "Could not get comments for video m0VqLTtnt4w\n",
      "Could not get comments for video RB_C8goBKuE\n",
      "Could not get comments for video 5dgl2OHbO6g\n",
      "Could not get comments for video mtODIVbOyiM\n",
      "Getting video information from channel: CBS News\n",
      "Could not get comments for video _kHaLBCxIJ8\n",
      "Could not get comments for video genQJU2cG0o\n",
      "Getting video information from channel: MSNBC\n"
     ]
    }
   ],
   "source": [
    "api_keys = ['AIzaSyA4Sd1FkOSah19dL7cg7OuBUj9VBJiE2fE', 'AIzaSyB-4NIQtecQPbRX7TWKphThkb9_Brh2wL4']\n",
    "\n",
    "channel_df = get_channel_stats(youtube, channel_ids)\n",
    "# Initialize dataframes\n",
    "video_df = pd.DataFrame()\n",
    "comments_df = pd.DataFrame()\n",
    "comments_all_data_df = pd.DataFrame()\n",
    "\n",
    "for c in channel_df['channelName'].unique():\n",
    "    print(\"Getting video information from channel: \" + c)\n",
    "    playlist_id = channel_df.loc[channel_df['channelName']== c, 'playlistId'].iloc[0]\n",
    "    channel_id = channel_df.loc[channel_df['channelName']== c, 'channel_id'].iloc[0]  # Get the channel_id\n",
    "    video_ids = get_video_ids(youtube, playlist_id)\n",
    "# Split the video_ids list into two parts\n",
    "    split_point = len(video_ids) // 2\n",
    "    video_ids_parts = [video_ids[:split_point], video_ids[split_point:]]\n",
    "\n",
    "    \n",
    "\n",
    "# Define a function to fetch data for a given API key and video IDs\n",
    "\n",
    "\n",
    "# Loop through API keys and video ID parts\n",
    "    for api_key, video_ids_part in zip(api_keys, video_ids_parts):\n",
    "        video_data_part, comments_data_part, comments_combined_part = fetch_data(api_key, video_ids_part, channel_id)\n",
    "\n",
    "        # Append data for the current part to the respective dataframes\n",
    "        video_df = pd.concat([video_df, video_data_part], ignore_index=True)\n",
    "        comments_df = pd.concat([comments_df, comments_combined_part], ignore_index=True)\n",
    "        comments_all_data_df = pd.concat([comments_all_data_df, comments_data_part])\n",
    "\n",
    "playlist_df = get_playlists_info(youtube, channel_ids)\n",
    "\n",
    "channel_df = get_channel_stats(youtube, channel_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cee815cb-46e2-40d0-85a0-a56db0dd42ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with video statistics and comments from all channels\n",
    "\n",
    "# video_df = pd.DataFrame()\n",
    "# comments_df = pd.DataFrame()\n",
    "# comments_all_data_df = pd.DataFrame()\n",
    "\n",
    "# for c in channel_df['channelName'].unique():\n",
    "#     print(\"Getting video information from channel: \" + c)\n",
    "#     playlist_id = channel_df.loc[channel_df['channelName']== c, 'playlistId'].iloc[0]\n",
    "#     channel_id = channel_df.loc[channel_df['channelName']== c, 'channel_id'].iloc[0]  # Get the channel_id\n",
    "#     video_ids = get_video_ids(youtube, playlist_id)\n",
    "    \n",
    "#     # get video data\n",
    "#     video_data = get_video_details(youtube, video_ids, channel_id)  # Pass channel_id\n",
    "#     # get comment data\n",
    "#     #comments_data_df, comments_combined_df = get_comments(youtube, video_ids)\n",
    "\n",
    "#     # append video data together and comment data toghether\n",
    "#     video_df = video_df.append(video_data, ignore_index=True)\n",
    "#     comments_df = comments_df.append(comments_combined_df, ignore_index=True)\n",
    "#     comments_all_data_df = comments_all_data_df.append(comments_data_df)\n",
    "\n",
    "# playlist_df = get_playlists_info(youtube, channel_ids)\n",
    "\n",
    "# channel_df = get_channel_stats(youtube, channel_ids)\n",
    "\n",
    "# #captions_df = get_captions(youtube, video_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25755eef",
   "metadata": {},
   "source": [
    "## Importing the data into Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dceddedd-f561-44c5-be35-5add6ad0ed60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.connector\n",
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Snowflake connection parameters\n",
    "snowflake_user = 'FURNITUREWALAABBAS'\n",
    "snowflake_password = 'Abba$123'\n",
    "snowflake_account = 'jrnvcvi-sw72415'\n",
    "snowflake_database = 'YOUTUBE_LLM'\n",
    "snowflake_schema = 'PUBLIC'\n",
    "#snowflake_warehouse = 'your_warehouse'\n",
    "\n",
    "# Create a Snowflake connection\n",
    "conn = snowflake.connector.connect(\n",
    "    user=snowflake_user,\n",
    "    password=snowflake_password,\n",
    "    account=snowflake_account,\n",
    "    #warehouse=snowflake_warehouse,\n",
    "    database=snowflake_database,\n",
    "    schema=snowflake_schema\n",
    ")\n",
    "\n",
    "# Create a cursor object\n",
    "cur = conn.cursor()\n",
    "\n",
    "# cur.execute(\"CREATE OR REPLACE TABLE YOUTUBE_LLM.PUBLIC.VIDEOS ( \\\n",
    "#     channel_id STRING, \\\n",
    "#     video_id STRING,\\\n",
    "#     channelTitle STRING,\\\n",
    "#     title STRING,\\\n",
    "#     description STRING,\\\n",
    "#     tags VARIANT, \\\n",
    "#     publishedAt TIMESTAMP_NTZ,\\\n",
    "#     defaultAudioLanguage STRING, \\\n",
    "#     viewCount FLOAT,\\\n",
    "#     likeCount FLOAT,\\\n",
    "#     favouriteCount FLOAT,\\\n",
    "#     commentCount FLOAT,\\\n",
    "#     duration STRING, \\\n",
    "#     definition STRING,\\\n",
    "#     caption BOOLEAN,\\\n",
    "#     pushblishDayName STRING,\\\n",
    "#     durationSecs FLOAT,\\\n",
    "#     tagsCount INTEGER,\\\n",
    "#     likeRatio FLOAT,\\\n",
    "#     commentRatio FLOAT,\\\n",
    "#     titleLength INTEGER)\" ) \n",
    "\n",
    "# # LOAD TABLE VIDEOS\n",
    " \n",
    "# write_pandas(conn, video_df, 'VIDEOS', quote_identifiers= False)\n",
    "\n",
    "# cur.execute(\"CREATE OR REPLACE TABLE YOUTUBE_LLM.PUBLIC.COMMENTS ( \\\n",
    "#     VIDEO_ID VARCHAR(1000000) ,  \\\n",
    "#     COMMENTS VARCHAR(16777216),  \\\n",
    "#     LIKECOUNT NUMBER(38, 0), \\\n",
    "#     AUTHORDISPLAYNAME VARCHAR(500), \\\n",
    "#     AUTHORPROFILEIMAGEURL VARCHAR(10000),\\\n",
    "#     AUTHORCHANNELURL VARCHAR(10000),\\\n",
    "#     AUTHORCHANNELID VARCHAR(10000),\\\n",
    "#     CHANNELID VARCHAR(10000),\\\n",
    "#     CANRATE BOOLEAN,\\\n",
    "#     VIEWERRATING VARCHAR(10000),\\\n",
    "#     PUBLISHEDAT TIMESTAMP_NTZ(9))\")\n",
    "\n",
    "\n",
    "# write_pandas(conn, comments_all_data_df, 'COMMENTS', quote_identifiers= False)\n",
    "\n",
    "# CREATING TABLE COMMENTS_COMBINED\n",
    "\n",
    "# cur.execute(\"create or replace TABLE YOUTUBE_LLM.PUBLIC.COMMENTS_COMBINED ( \\\n",
    "# \tVIDEO_ID VARCHAR(10000000),\\\n",
    "# \tCOMMENTS VARIANT);\")\n",
    "\n",
    "# write_pandas(conn, comments_df, 'COMMENTS_COMBINED', quote_identifiers= False)\n",
    "\n",
    "# cur.execute(\"create or replace TABLE YOUTUBE_LLM.PUBLIC.CHANNELS ( \\\n",
    "# \tCHANNELNAME VARCHAR(16777216),\\\n",
    "# \tCHANNEL_ID VARCHAR(16777216),\\\n",
    "# \tSUBSCRIBERS NUMBER(38,0),\\\n",
    "# \tVIEWS NUMBER(38,0),\\\n",
    "# \tTOTALVIDEOS NUMBER(38,0),\\\n",
    "# \tPLAYLISTID VARCHAR(16777216) )\")\n",
    "\n",
    "\n",
    "# write_pandas(conn, channel_df, 'CHANNELS', quote_identifiers= False)\n",
    "\n",
    "# CREATING TABLE PLAYLIST\n",
    "\n",
    "cur.execute(\"create or replace TABLE YOUTUBE_LLM.PUBLIC.PLAYLIST (\\\n",
    "\tPLAYLIST_ID VARCHAR(10000000),\\\n",
    "\tTITLE VARCHAR(10000000),\\\n",
    "\tDESCRIPTION VARCHAR(10000000),\\\n",
    "\tPUBLISHEDAT TIMESTAMP_NTZ(9),\\\n",
    "\tCHANNELID VARCHAR(10000000),\\\n",
    "\tCHANNELTITLE VARCHAR(10000000),\\\n",
    "\tDEFAULTLANGUAGE VARCHAR(10000000),\\\n",
    "\tTHUMBNAILURL VARCHAR(10000000));\")\n",
    "\n",
    "\n",
    "write_pandas(conn, playlist_df, 'PLAYLIST', quote_identifiers= False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the table name\n",
    "# table_name = 'CHANNELS'\n",
    "\n",
    "# Create an internal stage (temporary storage for data)\n",
    "# stage_name = 'STAGING'\n",
    "# cur.execute(f'CREATE OR REPLACE STAGE {stage_name}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Upload the DataFrame to Snowflake stage\n",
    "# csv_filename = 'video_data1.csv'\n",
    "# video_df1.to_csv(csv_filename, index=False)\n",
    "#cur.execute(r\"PUT file:///C:\\Users\\furni\\youtube-api-analysis\\video_data1.csv @STAGING_TABLES AUTO_COMPRESS=TRUE\")\n",
    "\n",
    "# Copy data from the stage into a Snowflake table\n",
    "\n",
    "    \n",
    "\n",
    "# # Copy data from the stage into the Snowflake table\n",
    "# csv_filepath = r'C:\\\\Users\\\\furni\\\\youtube-api-analysis\\\\video_data1.csv'\n",
    "# copy_query = f'''COPY INTO VIDEOS FROM 'file://{csv_filepath}' FILE_FORMAT = (TYPE = CSV SKIP_HEADER = 1)'''\n",
    "# cur.execute(copy_query)\n",
    "\n",
    "# Commit the changes\n",
    "conn.commit()\n",
    "\n",
    "# Close the cursor and connection\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "#print(f'DataFrame has been successfully uploaded as table: {table_name}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
