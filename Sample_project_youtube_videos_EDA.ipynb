{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc88f6c2-0fe6-4809-bd30-668aff01e30c",
   "metadata": {},
   "source": [
    "# Youtube LLM Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21458ddf-101b-4ecd-8ee8-3cf6655eb46b",
   "metadata": {},
   "source": [
    "## 1.4. Dataset\n",
    "\n",
    "### Data selection\n",
    "\n",
    "As this project is particularly focused on data science channels, I found that not many readily available datasets online are suitable for this purpose. The 2 alternative datasets I found are:\n",
    "\n",
    "- [The top trending Youtube videos on Kaggle](https://www.kaggle.com/rsrishav/youtube-trending-video-dataset): This dataset contains several months of data on daily trending YouTube videos for several countries. There are up to 200 trending videos per day. However, this dataset is not fit for this project because the trending videos are about a wide range of topics that are not necessarily related to data science. \n",
    "\n",
    "- Another dataset is obtained from this [Github repo](https://gitlab.com/thebrahminator/Youtube-View-Predictor) of Vishwanath Seshagiri, which is the metadata of 0.5M+ YouTube videos along with their channel data. There is no clear documentation on how this dataset was created, but a quick look at the datasets in the repository suggested that the data was obtained using keyword search of popular keywords such as \"football\" or \"science\". There are also some relevant keywords such as \"python\". However, I decided not to use these datasets because they don't contain data for the channels I am interested in.\n",
    "\n",
    "I created my own dataset using the [Google Youtube Data API version 3.0](https://developers.google.com/youtube/v3). The exact steps of data creation is presented in section *2. Data Creation* below.\n",
    "\n",
    "### Data limitations\n",
    "\n",
    "The dataset is a real-world dataset and suitable for the research. However, the selection of the top 10 Youtube channels to include in the research is purely based on my knowledge of the channels in data science field and might not be accurate. My definition is \"popular\" is only based on subscriber count but there are other metrics that could be taken into consideration as well (e.g. views, engagement). The top 10 also seems arbitrary given the plethora of channels on Youtube. There might be smaller channels that might also very interesting to look into, which could be the next step of this project.\n",
    "\n",
    "### Ethics of data source\n",
    "\n",
    "According to [Youtube API's guide](https://developers.google.com/youtube/v3/getting-started), the usage of Youtube API is free of charge given that your application send requests within a quota limit. \"The YouTube Data API uses a quota to ensure that developers use the service as intended and do not create applications that unfairly reduce service quality or limit access for others. \" The default quota allocation for each application is 10,000 units per day, and you could request additional quota by completing a form to YouTube API Services if you reach the quota limit.\n",
    "\n",
    "Since all data requested from Youtube API is public data (which everyone on the Internet can see on Youtube), there is no particular privacy issues as far as I am concerned. In addition, the data is obtained only for research purposes in this case and not for any commercial interests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2ba6bd0-2c70-42d5-ab1d-9ebff01c62ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\furni\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.0' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "c:\\Users\\furni\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:62: UserWarning: Pandas requires version '1.3.4' or newer of 'bottleneck' (version '1.3.2' currently installed).\n",
      "  from pandas.core import (\n",
      "c:\\Users\\furni\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dateutil import parser\n",
    "import isodate\n",
    "from datetime import datetime, timedelta\n",
    "from googleapiclient.errors import HttpError \n",
    "\n",
    "# Data visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\", color_codes=True)\n",
    "\n",
    "# Google API\n",
    "from googleapiclient.discovery import build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab8a223f-2420-421f-8647-344591a19e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\furni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\furni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# NLP libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be282b2-b566-4587-97d1-71b680cf6b92",
   "metadata": {},
   "source": [
    "# 2. Data creation with Youtube API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400f8704-cfe4-495b-af35-bb5dfb677c6e",
   "metadata": {},
   "source": [
    "I first created a project on Google Developers Console, then requested an authorization credential (API key). Afterwards, I enabled Youtube API for my application, so that I can send API requests to Youtube API services. Then, I went on Youtube and checked the channel ID of each of the channels that I would like to include in my research scope (using their URLs). Then I created the functions for getting the channel statistics via the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "359fc781",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = 'AIzaSyB-4NIQtecQPbRX7TWKphThkb9_Brh2wL4' \n",
    "#api_key = 'AIzaSyA4Sd1FkOSah19dL7cg7OuBUj9VBJiE2fE'\n",
    "api_keys = ['AIzaSyDux3Qn4l6wcGMsj729rPiz_wkUv9ZeEi8', 'AIzaSyB-4NIQtecQPbRX7TWKphThkb9_Brh2wL4']\n",
    "# channel_ids = ['UCtYLUTtgS3k1Fg4y5tAhLbw', # Statquest \n",
    "#                'UCCezIgC97PvUuR4_gbFUs5g', # Corey Schafer\n",
    "#                'UCfzlCWGWYyIQ0aLC5w48gBQ', # Sentdex\n",
    "#                'UCNU_lfiiWBdtULKOw6X0Dig', # Krish Naik\n",
    "#                'UCzL_0nIe8B4-7ShhVPfJkgw', # DatascienceDoJo\n",
    "#                'UCLLw7jmFsvfIVaUFsLs8mlQ', # Luke Barousse \n",
    "#                'UCiT9RITQ9PW6BhXK0y2jaeg', # Ken Jee\n",
    "#                'UC7cs8q-gJRlGwj4A8OmCmXg', # Alex the analyst\n",
    "#                'UC2UXDak6o7rBm23k3Vv5dww', # Tina Huang\n",
    "#               ]\n",
    "\n",
    "channel_ids = [\n",
    "     'UCupvZG-5ko_eiXAupbDfxWw',  # CNN\n",
    "    #  'UCXIJgqnII2ZOINSWNOGFThA',  # FOX NEWS\n",
    "    # 'UCaXkIU1QidjPwiAYu6GcHjg',  # MSNBC\n",
    "    #'UCBi2mrWuNuyYy4gbM6fU18Q',  # ABC NEWS\n",
    "    #  'UC8p1vwvWtl6T73JiExfWs1g',  # CBS NEWS\n",
    "]\n",
    "\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e528263d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_stats(youtube, channel_ids):\n",
    "    \"\"\"\n",
    "    Get channel statistics: title, subscriber count, view count, video count, upload playlist\n",
    "    Params:\n",
    "    \n",
    "    youtube: the build object from googleapiclient.discovery\n",
    "    channels_ids: list of channel IDs\n",
    "    \n",
    "    Returns:\n",
    "    Dataframe containing the channel statistics for all channels in the provided list: title, subscriber count, view count, video count, upload playlist\n",
    "    \n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    request = youtube.channels().list(\n",
    "                part='snippet,contentDetails,statistics',\n",
    "                id=','.join(channel_ids))\n",
    "    response = request.execute() \n",
    "    \n",
    "    for i in range(len(response['items'])):\n",
    "        data = dict(channelName = response['items'][i]['snippet']['title'],\n",
    "                    channel_id=channel_ids[i],\n",
    "                    subscribers = response['items'][i]['statistics']['subscriberCount'],\n",
    "                    views = response['items'][i]['statistics']['viewCount'],\n",
    "                    totalVideos = response['items'][i]['statistics']['videoCount'],\n",
    "                    playlistId = response['items'][i]['contentDetails']['relatedPlaylists']['uploads'])\n",
    "        all_data.append(data)\n",
    "    \n",
    "    return pd.DataFrame(all_data)\n",
    "\n",
    "def get_video_ids(youtube, playlist_id):\n",
    "    \"\"\"\n",
    "    Get list of video IDs of all videos in the given playlist for the last month past(30 days) \n",
    "    Params:\n",
    "    \n",
    "    youtube: the build object from googleapiclient.discovery\n",
    "    playlist_id: playlist ID of the channel\n",
    "    \n",
    "    Returns:\n",
    "    List of video IDs of all videos in the playlist\n",
    "    \n",
    "    \"\"\"\n",
    "    one_month_ago = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "    \n",
    "    request = youtube.playlistItems().list(\n",
    "                part='contentDetails',\n",
    "                playlistId=playlist_id,\n",
    "                maxResults=50)\n",
    "    response = request.execute()\n",
    "    \n",
    "    video_ids = []\n",
    "    \n",
    "    for i in range(len(response['items'])):\n",
    "        try:\n",
    "            video_published_at = response['items'][i]['contentDetails']['videoPublishedAt']\n",
    "\n",
    "            # Check if the video was published in the past month\n",
    "            if video_published_at >= one_month_ago:\n",
    "                video_ids.append(response['items'][i]['contentDetails']['videoId'])\n",
    "        except KeyError:\n",
    "            continue  # Skip this item if 'videoPublishedAt' is not found\n",
    "\n",
    "\n",
    "\n",
    "    next_page_token = response.get('nextPageToken')\n",
    "    more_pages = True\n",
    "    \n",
    "    while more_pages:\n",
    "        if next_page_token is None:\n",
    "            more_pages = False\n",
    "        else:\n",
    "            request = youtube.playlistItems().list(\n",
    "                        part='contentDetails',\n",
    "                        playlistId = playlist_id,\n",
    "                        maxResults = 50,\n",
    "                        pageToken = next_page_token\n",
    "                        )\n",
    "            response = request.execute()\n",
    "\n",
    "\n",
    "            for i in range(len(response['items'])):\n",
    "                try:\n",
    "                    video_published_at = response['items'][i]['contentDetails']['videoPublishedAt']\n",
    "\n",
    "            # Check if the video was published in the past month\n",
    "                    if video_published_at >= one_month_ago:\n",
    "                        video_ids.append(response['items'][i]['contentDetails']['videoId'])\n",
    "                except KeyError:\n",
    "                    continue  # Skip this item if 'videoPublishedAt' is not found\n",
    "    \n",
    "            \n",
    "            next_page_token = response.get('nextPageToken')\n",
    "    return video_ids\n",
    "\n",
    "\n",
    "\n",
    "def get_video_details(youtube, video_ids, channel_id, playlist_id):\n",
    "    \"\"\"\n",
    "    Get video statistics of all videos with given IDs\n",
    "    Params:\n",
    "    \n",
    "    youtube: the build object from googleapiclient.discovery\n",
    "    video_ids: list of video IDs\n",
    "    channel_id: ID of the channel\n",
    "    \n",
    "    Returns:\n",
    "    Dataframe with statistics of videos, i.e.:\n",
    "        'channel_id', 'channelTitle', 'title', 'description', 'tags', 'publishedAt'\n",
    "        'viewCount', 'likeCount', 'favoriteCount', 'commentCount'\n",
    "        'duration', 'definition', 'caption'\n",
    "    \"\"\"\n",
    "        \n",
    "    all_video_info = []\n",
    "    \n",
    "    for i in range(0, len(video_ids), 50):\n",
    "        request = youtube.videos().list(\n",
    "            part=\"snippet,contentDetails,statistics\",\n",
    "            id=','.join(video_ids[i:i+50])\n",
    "        )\n",
    "        response = request.execute() \n",
    "\n",
    "        for video in response['items']:\n",
    "            stats_to_keep = {'snippet': ['channelTitle', 'title', 'description', 'tags', 'publishedAt','defaultAudioLanguage'],\n",
    "                             'statistics': ['viewCount', 'likeCount', 'favouriteCount', 'commentCount'],\n",
    "                             'contentDetails': ['duration', 'definition', 'caption']\n",
    "                            }\n",
    "            video_info = {}\n",
    "            video_info['channel_id'] = channel_id  # Add channel_id to the DataFrame\n",
    "            video_info['video_id'] = video['id']\n",
    "            video_info['playlist_id'] = playlist_id\n",
    "\n",
    "            for k in stats_to_keep.keys():\n",
    "                for v in stats_to_keep[k]:\n",
    "                    try:\n",
    "                        video_info[v] = video[k][v]\n",
    "                    except:\n",
    "                        video_info[v] = None\n",
    "\n",
    "            all_video_info.append(video_info)\n",
    "            \n",
    "    return pd.DataFrame(all_video_info)\n",
    "\n",
    "\n",
    "\n",
    "def get_playlists_info(youtube, channel_ids):\n",
    "\n",
    "\n",
    "    all_playlist_data = []\n",
    "\n",
    "    \"\"\"\n",
    "    Retreiving Playlist data for all the channels\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    for channel_id in channel_ids:\n",
    "        request = youtube.playlists().list(\n",
    "            part=\"snippet\",\n",
    "            channelId=channel_id,\n",
    "            maxResults=50  # Adjust the maximum number of playlists to retrieve if needed\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        for playlist in response.get(\"items\", []):\n",
    "            playlist_data = dict(\n",
    "                playlist_id=playlist[\"id\"],\n",
    "                title=playlist[\"snippet\"][\"title\"],\n",
    "                description=playlist[\"snippet\"][\"description\"],\n",
    "                publishedAt=playlist[\"snippet\"][\"publishedAt\"],\n",
    "                channelId=playlist[\"snippet\"][\"channelId\"],\n",
    "                channelTitle=playlist[\"snippet\"][\"channelTitle\"],\n",
    "                defaultLanguage=playlist[\"snippet\"].get(\"defaultLanguage\"),\n",
    "                thumbnailUrl=playlist[\"snippet\"][\"thumbnails\"][\"default\"][\"url\"]\n",
    "            )\n",
    "            all_playlist_data.append(playlist_data)\n",
    "    return pd.DataFrame(all_playlist_data)\n",
    "\n",
    "\n",
    "def get_captions(youtube, video_ids):\n",
    "    caption_list = []\n",
    "\n",
    "    for video_i in video_ids:\n",
    "            captions = youtube.captions().list(\n",
    "            part=\"snippet\",\n",
    "            videoId=video_i\n",
    "        ).execute()\n",
    "\n",
    "        # List to store comments as dictionaries\n",
    "            \n",
    "\n",
    "        # Extract comments and append them to the list\n",
    "            for caption in captions[\"items\"]:\n",
    "                snippet = caption[\"snippet\"]\n",
    "                caption_dict = {\n",
    "        \"videoId\": snippet[\"videoId\"],\n",
    "        \"lastUpdated\": snippet[\"lastUpdated\"],\n",
    "        \"trackKind\": snippet[\"trackKind\"],\n",
    "        \"language\": snippet[\"language\"],\n",
    "        \"name\": snippet[\"name\"],\n",
    "        \"audioTrackType\": snippet[\"audioTrackType\"],\n",
    "        \"status\": snippet[\"status\"]\n",
    "    }\n",
    "                caption_list.append(caption_dict)\n",
    "    return(pd.DataFrame(caption_list))\n",
    "\n",
    "def get_comments(youtube, video_ids):\n",
    "    \"\"\"\n",
    "    Get top level comments as text from all videos with given IDs (only the first 50 comments per video due to quote limit of Youtube API)\n",
    "    Params:\n",
    "    \n",
    "    youtube: the build object from googleapiclient.discovery\n",
    "    video_ids: list of video IDs\n",
    "    \n",
    "    Returns:\n",
    "    Dataframe with video IDs and associated top level comment in text.\n",
    "    \n",
    "    \"\"\"\n",
    "    all_comments = []\n",
    "    all_comments_data = []    \n",
    "    for video_id in video_ids:\n",
    "        comments_in_video_info = {}\n",
    "        try:   \n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet,replies\",\n",
    "                videoId=video_id\n",
    "            )\n",
    "            response = request.execute()\n",
    "            \n",
    "            comments_in_video= []\n",
    "            comments_in_video_info = {}\n",
    "            for comment in response['items'][:50]:\n",
    "                comment_text = comment['snippet']['topLevelComment']['snippet']['textOriginal']\n",
    "        \n",
    "                # Append the comment text to the list\n",
    "                comments_in_video.append(comment_text)\n",
    "                comments_data = {'video_id': video_id, \n",
    "                                'comments': comment_text,\n",
    "                                'likeCount': comment['snippet']['topLevelComment']['snippet']['likeCount'],\n",
    "                                'authorDisplayName': comment['snippet']['topLevelComment']['snippet']['authorDisplayName'],\n",
    "                                'authorProfileImageUrl': comment['snippet']['topLevelComment']['snippet']['authorProfileImageUrl'],\n",
    "                                'authorChannelUrl': comment['snippet']['topLevelComment']['snippet']['authorChannelUrl'],\n",
    "                                'authorChannelId': comment['snippet']['topLevelComment']['snippet']['authorChannelId']['value'],\n",
    "                                'channelId': comment['snippet']['topLevelComment']['snippet']['channelId'],\n",
    "                                'canRate': comment['snippet']['topLevelComment']['snippet']['canRate'],\n",
    "                                'viewerRating': comment['snippet']['topLevelComment']['snippet']['viewerRating'],\n",
    "                                'publishedAt': comment['snippet']['topLevelComment']['snippet']['publishedAt']\n",
    "                                \n",
    "                                                            \n",
    "                                }\n",
    "                all_comments_data.append(comments_data)\n",
    "            comments_in_video_info = {'video_id': video_id, 'comments': comments_in_video}\n",
    "            \n",
    "\n",
    "\n",
    "        except: \n",
    "            # When error occurs - most likely because comments are disabled on a video\n",
    "            print('Could not get comments for video ' + video_id)\n",
    "\n",
    "\n",
    "\n",
    "        all_comments.append(comments_in_video_info)\n",
    "\n",
    "            \n",
    "                \n",
    "        \n",
    "        # Create a dictionary for each comment and append it to the list\n",
    "                # comment_info = {'video_id': video_id, 'comment': comment_text}\n",
    "                # comments_in_video_info.append(comment_info)\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "    return pd.DataFrame(all_comments_data) , pd.DataFrame(all_comments)   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_ids_test(youtube, playlist_id):\n",
    "    \"\"\"\n",
    "    Get list of video IDs of all videos in the given playlist for the last month past(30 days) \n",
    "    Params:\n",
    "    \n",
    "    youtube: the build object from googleapiclient.discovery\n",
    "    playlist_id: playlist ID of the channel\n",
    "    \n",
    "    Returns:\n",
    "    List of video IDs of all videos in the playlist\n",
    "    \n",
    "    \"\"\"\n",
    "    one_month_ago = (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "    \n",
    "    request = youtube.playlistItems().list(\n",
    "                part='contentDetails',\n",
    "                playlistId=playlist_id,\n",
    "                maxResults=50)\n",
    "    response = request.execute()\n",
    "    \n",
    "    video_ids = []\n",
    "    \n",
    "    \n",
    "    for i in range(len(response['items'])):\n",
    "        video_published_at = response['items'][i]['contentDetails']['videoPublishedAt']\n",
    "    \n",
    "    # Check if the video was published in the past month\n",
    "    if video_published_at >= one_month_ago:\n",
    "        video_ids.append(response['items'][i]['contentDetails']['videoId'])\n",
    "        \n",
    "    return video_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Um2slptZ8_c']\n",
      "['Um2slptZ8_c', '8Y8Hg_j9Odg']\n",
      "['Um2slptZ8_c', '8Y8Hg_j9Odg', 'qsRqonFUaoA']\n",
      "['Um2slptZ8_c', '8Y8Hg_j9Odg', 'qsRqonFUaoA', 'z0pakD5Efww']\n",
      "['Um2slptZ8_c', '8Y8Hg_j9Odg', 'qsRqonFUaoA', 'z0pakD5Efww', 'uCohvZAQauQ']\n",
      "['Um2slptZ8_c', '8Y8Hg_j9Odg', 'qsRqonFUaoA', 'z0pakD5Efww', 'uCohvZAQauQ', 'xPlBPJy3bPA']\n",
      "['Um2slptZ8_c', '8Y8Hg_j9Odg', 'qsRqonFUaoA', 'z0pakD5Efww', 'uCohvZAQauQ', 'xPlBPJy3bPA', 'Uo2MQAVM0Ko']\n"
     ]
    }
   ],
   "source": [
    "one_month_ago = (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "    \n",
    "request = youtube.playlistItems().list(\n",
    "            part='contentDetails',\n",
    "            playlistId='PL6XRrncXkMaVg2Xd4g2icI4M36-eAozy0',\n",
    "            maxResults=50)\n",
    "response = request.execute()\n",
    "\n",
    "video_ids = []\n",
    "    \n",
    "for i in range(len(response['items'])):\n",
    "    try:\n",
    "        video_published_at = response['items'][i]['contentDetails']['videoPublishedAt']\n",
    "\n",
    "        # Check if the video was published in the past month\n",
    "        if video_published_at >= one_month_ago:\n",
    "            video_ids.append(response['items'][i]['contentDetails']['videoId'])\n",
    "    except KeyError:\n",
    "        continue  # Skip this item if 'videoPublishedAt' is not found\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507bec49",
   "metadata": {},
   "source": [
    "## CREATING DATAFRAMES FOR ALL THE TABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa92567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(api_key, video_ids, channel_id, playlist_id):\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "    \n",
    "\n",
    "    # Get video data for the current chunk of video IDs\n",
    "    video_data = get_video_details(youtube, video_ids, channel_id, playlist_id)\n",
    "\n",
    "    # Get comment data for the current chunk of video IDs\n",
    "    #comments_data_df, comments_combined_df = get_comments(youtube, video_ids)\n",
    "\n",
    "    return video_data\n",
    "    # return video_data, comments_data_df, comments_combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  channelName                channel_id subscribers        views totalVideos  \\\n",
      "0         CNN  UCupvZG-5ko_eiXAupbDfxWw    15800000  15043205569      162438   \n",
      "\n",
      "                 playlistId  \n",
      "0  UUupvZG-5ko_eiXAupbDfxWw  \n"
     ]
    }
   ],
   "source": [
    "api_keys = ['AIzaSyA4Sd1FkOSah19dL7cg7OuBUj9VBJiE2fE', 'AIzaSyB-4NIQtecQPbRX7TWKphThkb9_Brh2wL4']\n",
    "#api_keys = ['AIzaSyDux3Qn4l6wcGMsj729rPiz_wkUv9ZeEi8', 'AIzaSyB-4NIQtecQPbRX7TWKphThkb9_Brh2wL4']\n",
    "channel_df = get_channel_stats(youtube, channel_ids)\n",
    "print(channel_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PL6XRrncXkMaVg2Xd4g2icI4M36-eAozy0\n",
      "PL6XRrncXkMaU55GiCvv416NR2qBD_xbmf\n",
      "PL6XRrncXkMaW9CdmnrS4NVWKavtqRO8hT\n",
      "PL6XRrncXkMaVXAutoJ8D2RDKAz_XufaFm\n",
      "PL6XRrncXkMaXpv1ZA2l3jwnhbajEEROUe\n",
      "PL6XRrncXkMaW8rqNW6ddCsT6SEosWF4W2\n",
      "PL6XRrncXkMaWdcHpyKh3fcAsf6aDOQOnM\n",
      "PL6XRrncXkMaWIJy9reBfgOQkPMfPt-KNl\n",
      "PL6XRrncXkMaWuCgRkfO9o_B2odKQxTBI4\n",
      "PL6XRrncXkMaVKJiRkKEDLbOQ7kaAIBR1t\n",
      "PL6XRrncXkMaUJEgNM9QbfAulA2u684ON0\n",
      "PL6XRrncXkMaVcAyYoXByXNtFJ1nxRg6J7\n",
      "PL6XRrncXkMaUZb-dIuCXa1csnpyVvoErY\n",
      "PL6XRrncXkMaWmz7BIhhVBwL6kG3psLUuw\n",
      "PL6XRrncXkMaU4yqgPbhqblbXacFO1TjXY\n",
      "PL6XRrncXkMaWsHRkoIp3Q9WcQb4zhR6_t\n",
      "PL6XRrncXkMaV5_rYaLzWHhYke_03L4ILC\n",
      "PL6XRrncXkMaXedndSn9yRspW6X8YZ8HQ1\n",
      "PL6XRrncXkMaUFpKf-Qw2DYq6H7hGmDxvu\n",
      "PL6XRrncXkMaUeGtwPgP7xJXdCMAlQhsP_\n",
      "PL6XRrncXkMaXU3bMhinFvfq4DM4G0tlVZ\n",
      "PL6XRrncXkMaXgqq0hpakOLsopUNKn3qkG\n",
      "PL6XRrncXkMaWfaCwqxFbw6oZraf_Cct7F\n",
      "PL6XRrncXkMaV165eXa5TdI8hYW49bM3vd\n",
      "PL6XRrncXkMaWNjFNasXgmPKCWNav0hyUd\n",
      "PL6XRrncXkMaXcuPCbB4myVD7oj08b3Ndi\n",
      "PL6XRrncXkMaWROpnwl8zYUfJxXarXf4be\n",
      "PL6XRrncXkMaURgZa1AEsOsjHWMwIGk6HO\n",
      "PL6XRrncXkMaVGQM7ra02ystnTt_PExEgr\n",
      "PL6XRrncXkMaXVLkW2M3rZCf1lFgUUGuqD\n",
      "PL6XRrncXkMaUs6ngihaWNCnoVBJWz0nij\n",
      "PL6XRrncXkMaVjy9R54e2ZIZ5cpxBRA1IC\n",
      "PL6XRrncXkMaXQ2Lef8-WxTsR-DHHUi7lM\n",
      "PL6XRrncXkMaU4WjYUV3AJj6dqeYYqz8oP\n",
      "PL6XRrncXkMaWDo6NkCBk0SA1BpBsLO9EW\n",
      "PL6XRrncXkMaX9mLW3lR_q4r_c-zU2s0nW\n",
      "PL6XRrncXkMaUTlptwbUFcIhCqFXG5Klj7\n",
      "PL6XRrncXkMaUePR7EOfMNJoJ-1VIf0fRc\n",
      "PL6XRrncXkMaUDoHYLRfgyBptsGVv9ekAm\n",
      "PL6XRrncXkMaWTtcQYZ4SHorWnYm1Jfiy7\n",
      "PL6XRrncXkMaV0uvkjmnk1-MT5jZziqcLz\n",
      "PL6XRrncXkMaUOQ1eMN5vFRc1KXE8AIn31\n",
      "PL6XRrncXkMaVuqD8GRJft35lg3y6NiTmN\n",
      "PL6XRrncXkMaXKhaVcEHHkrEMHeA2Us65N\n",
      "PL6XRrncXkMaUlx0CD8d4OG2Q_f5LKSbrW\n",
      "PL6XRrncXkMaV71jO86o16UCZvMzX8g1yK\n",
      "PL6XRrncXkMaVJ43eIITnIKD9odjmJKmXp\n",
      "PL6XRrncXkMaWUR5k6HBxZR-WzWsXDSmbz\n",
      "PL6XRrncXkMaXUslfXVzn1YbP5OcjZOo8Y\n",
      "PL6XRrncXkMaV1I4To_PQvdnL4ywP8XbwZ\n",
      "PLlTLHnxSVuIyMU4Q4I8NsLAVK1iNANGW9\n",
      "PLlTLHnxSVuIxR3JD2klrnA_X_k6QCBk9b\n",
      "PLlTLHnxSVuIwXMsGsYfesxoTMI2SpJ5mh\n",
      "PLlTLHnxSVuIwp5OShRSMCWmGULAKpI4kO\n",
      "PLlTLHnxSVuIyFIuTfkdHfUZ3VGimphU5i\n",
      "PLlTLHnxSVuIxhwKS4oC3PGeWEqGtmaSKy\n",
      "PLlTLHnxSVuIzqGW9CCVZgxLwVWovgBDZL\n",
      "PLlTLHnxSVuIzZJVdMeayCAyJ_wN0i6DDM\n",
      "PLlTLHnxSVuIw1RqZjD5RlzWL_N5umiqRp\n",
      "PLlTLHnxSVuIx0jv0z2ZMu3oQ86tHvMH7c\n",
      "PLlTLHnxSVuIyogstDWb502u95J4JMsA8G\n",
      "PLlTLHnxSVuIxb6JICY6CMVn5tLSKyEB-g\n",
      "PLlTLHnxSVuIzcH0PCBW0sCxPqUpETfxf7\n",
      "PLlTLHnxSVuIycbO7uttsCrmOUpDNFuTbf\n",
      "PLlTLHnxSVuIyN7GJL7UuhudKFAs-Bj3-A\n",
      "PLlTLHnxSVuIzd1ghElFP7FqvcCR_mtPPP\n",
      "PLlTLHnxSVuIzluqTOZxl1HGyCz2LEW1ck\n",
      "PLlTLHnxSVuIyCAx4TDbyonXJ_hEVPqT5J\n",
      "PLlTLHnxSVuIwVCP8pLUxhs8wc3Dqw8kaN\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19596/1596582848.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mplaylist_id\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mplaylist_ids\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplaylist_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mvideo_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_video_ids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myoutube\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplaylist_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mall_video_ids\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvideo_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_video_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19596/1300261278.py\u001b[0m in \u001b[0;36mget_video_ids\u001b[1;34m(youtube, playlist_id)\u001b[0m\n\u001b[0;32m     75\u001b[0m                         \u001b[0mpageToken\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_page_token\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m                         )\n\u001b[1;32m---> 77\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\furni\\anaconda3\\lib\\site-packages\\googleapiclient\\_helpers.py\u001b[0m in \u001b[0;36mpositional_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[0mpositional_parameters_enforcement\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mPOSITIONAL_WARNING\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mpositional_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\furni\\anaconda3\\lib\\site-packages\\googleapiclient\\http.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, http, num_retries)\u001b[0m\n\u001b[0;32m    921\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    922\u001b[0m         \u001b[1;31m# Handle retries for server-side errors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 923\u001b[1;33m         resp, content = _retry_request(\n\u001b[0m\u001b[0;32m    924\u001b[0m             \u001b[0mhttp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    925\u001b[0m             \u001b[0mnum_retries\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\furni\\anaconda3\\lib\\site-packages\\googleapiclient\\http.py\u001b[0m in \u001b[0;36m_retry_request\u001b[1;34m(http, num_retries, req_type, sleep, rand, uri, method, *args, **kwargs)\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m             \u001b[0mexception\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m             \u001b[0mresp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhttp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muri\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m         \u001b[1;31m# Retry on SSL errors and socket timeout errors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0m_ssl_SSLError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mssl_error\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\furni\\anaconda3\\lib\\site-packages\\httplib2\\__init__.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, uri, method, body, headers, redirections, connection_type)\u001b[0m\n\u001b[0;32m   1722\u001b[0m                     \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mb\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1723\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1724\u001b[1;33m                     (response, content) = self._request(\n\u001b[0m\u001b[0;32m   1725\u001b[0m                         \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauthority\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muri\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest_uri\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mredirections\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcachekey\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1726\u001b[0m                     )\n",
      "\u001b[1;32mc:\\Users\\furni\\anaconda3\\lib\\site-packages\\httplib2\\__init__.py\u001b[0m in \u001b[0;36m_request\u001b[1;34m(self, conn, host, absolute_uri, request_uri, method, body, headers, redirections, cachekey)\u001b[0m\n\u001b[0;32m   1442\u001b[0m             \u001b[0mauth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest_uri\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1444\u001b[1;33m         \u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conn_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest_uri\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1445\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1446\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mauth\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\furni\\anaconda3\\lib\\site-packages\\httplib2\\__init__.py\u001b[0m in \u001b[0;36m_conn_request\u001b[1;34m(self, conn, request_uri, method, body, headers)\u001b[0m\n\u001b[0;32m   1394\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1395\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1396\u001b[1;33m                 \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1397\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhttp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBadStatusLine\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhttp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mResponseNotReady\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1398\u001b[0m                 \u001b[1;31m# If we get a BadStatusLine on the first try then that means\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\furni\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1369\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1370\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1371\u001b[1;33m                 \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1372\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1373\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\furni\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mbegin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    317\u001b[0m         \u001b[1;31m# read until we get a non-100 response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m             \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    320\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\furni\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 280\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"iso-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    281\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"status line\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\furni\\anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    702\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 704\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    705\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    706\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\furni\\anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1239\u001b[0m                   \u001b[1;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1240\u001b[0m                   self.__class__)\n\u001b[1;32m-> 1241\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1242\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1243\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\furni\\anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1097\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1099\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1100\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1101\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "api_keys = ['AIzaSyA4Sd1FkOSah19dL7cg7OuBUj9VBJiE2fE', 'AIzaSyB-4NIQtecQPbRX7TWKphThkb9_Brh2wL4']\n",
    "all_video_ids = []\n",
    "channel_df = get_channel_stats(youtube, channel_ids)\n",
    "# Initialize dataframes\n",
    "video_df = pd.DataFrame()\n",
    "comments_df = pd.DataFrame()\n",
    "comments_all_data_df = pd.DataFrame()\n",
    "playlist_df = pd.DataFrame()\n",
    "playlist_df = get_playlists_info(youtube, channel_ids)\n",
    "playlist_ids = playlist_df['playlist_id'].tolist()\n",
    "for playlist_id in playlist_ids:\n",
    "    print(playlist_id)\n",
    "    video_ids = get_video_ids(youtube, playlist_id)\n",
    "    all_video_ids.extend(video_ids)\n",
    "print(all_video_ids)\n",
    "# for c in channel_df['channelName'].unique():\n",
    "#     print(\"Getting video information from channel: \" + c)\n",
    "#     playlist_id = channel_df.loc[channel_df['channelName']== c, 'playlistId'].iloc[0]\n",
    "#     channel_id = channel_df.loc[channel_df['channelName']== c, 'channel_id'].iloc[0]  # Get the channel_id\n",
    "#     print(playlist_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d27fb8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting video information from channel: CNN\n",
      "[[], []]\n",
      "[['sNULLdleZm0'], ['EFrJSRbQM8s']]\n",
      "AIzaSyA4Sd1FkOSah19dL7cg7OuBUj9VBJiE2fE ['sNULLdleZm0']\n",
      "AIzaSyB-4NIQtecQPbRX7TWKphThkb9_Brh2wL4 ['EFrJSRbQM8s']\n",
      "[[], []]\n",
      "[['hB0WgUxhNG0', 'sNULLdleZm0', 'ly7Hz96gDMc'], ['6Q7zUBYN3tE', 'HHWDbi3DZ0w', 'ifKH11DSP18']]\n",
      "AIzaSyA4Sd1FkOSah19dL7cg7OuBUj9VBJiE2fE ['hB0WgUxhNG0', 'sNULLdleZm0', 'ly7Hz96gDMc']\n",
      "AIzaSyB-4NIQtecQPbRX7TWKphThkb9_Brh2wL4 ['6Q7zUBYN3tE', 'HHWDbi3DZ0w', 'ifKH11DSP18']\n",
      "[[], []]\n",
      "[['pcL0oasrn2Y'], ['XLOy5ArGYQo', 'ly7Hz96gDMc']]\n",
      "AIzaSyA4Sd1FkOSah19dL7cg7OuBUj9VBJiE2fE ['pcL0oasrn2Y']\n",
      "AIzaSyB-4NIQtecQPbRX7TWKphThkb9_Brh2wL4 ['XLOy5ArGYQo', 'ly7Hz96gDMc']\n",
      "[[], []]\n",
      "[[], []]\n",
      "[['sNULLdleZm0', 'x3R-Vo6sNOA', 'PoahCWxhmE4', 'gMIT7Xa5QmQ', 'up5t8Akof18', '8p9qBAEdxsg', 'pYbeeh7syNk'], ['DrtEEYEZMNU', 'Jnd6-N0sdT4', 'J3GS2nchryE', 'BCXvnAa-gIk', 'XLOy5ArGYQo', 'pcL0oasrn2Y', '9DZS07KKjHg']]\n",
      "AIzaSyA4Sd1FkOSah19dL7cg7OuBUj9VBJiE2fE ['sNULLdleZm0', 'x3R-Vo6sNOA', 'PoahCWxhmE4', 'gMIT7Xa5QmQ', 'up5t8Akof18', '8p9qBAEdxsg', 'pYbeeh7syNk']\n",
      "AIzaSyB-4NIQtecQPbRX7TWKphThkb9_Brh2wL4 ['DrtEEYEZMNU', 'Jnd6-N0sdT4', 'J3GS2nchryE', 'BCXvnAa-gIk', 'XLOy5ArGYQo', 'pcL0oasrn2Y', '9DZS07KKjHg']\n",
      "[[], []]\n",
      "[[], []]\n",
      "[['9DZS07KKjHg', 'SxkWHg6YziA', 'BCXvnAa-gIk', 'J3GS2nchryE', 'Jnd6-N0sdT4', 'pYbeeh7syNk'], ['HHWDbi3DZ0w', '8p9qBAEdxsg', 'up5t8Akof18', 'gMIT7Xa5QmQ', 'PoahCWxhmE4', 'x3R-Vo6sNOA', 'sNULLdleZm0']]\n",
      "AIzaSyA4Sd1FkOSah19dL7cg7OuBUj9VBJiE2fE ['9DZS07KKjHg', 'SxkWHg6YziA', 'BCXvnAa-gIk', 'J3GS2nchryE', 'Jnd6-N0sdT4', 'pYbeeh7syNk']\n",
      "AIzaSyB-4NIQtecQPbRX7TWKphThkb9_Brh2wL4 ['HHWDbi3DZ0w', '8p9qBAEdxsg', 'up5t8Akof18', 'gMIT7Xa5QmQ', 'PoahCWxhmE4', 'x3R-Vo6sNOA', 'sNULLdleZm0']\n",
      "[[], []]\n",
      "[[], []]\n",
      "[[], []]\n",
      "[[], ['sOUP2o7kJEw']]\n",
      "AIzaSyB-4NIQtecQPbRX7TWKphThkb9_Brh2wL4 ['sOUP2o7kJEw']\n",
      "[[], ['PUP1FFWeq-Q']]\n",
      "AIzaSyB-4NIQtecQPbRX7TWKphThkb9_Brh2wL4 ['PUP1FFWeq-Q']\n",
      "[[], []]\n",
      "[[], ['PUP1FFWeq-Q']]\n",
      "AIzaSyB-4NIQtecQPbRX7TWKphThkb9_Brh2wL4 ['PUP1FFWeq-Q']\n",
      "[[], []]\n",
      "[['ly7Hz96gDMc'], ['BCXvnAa-gIk']]\n",
      "AIzaSyA4Sd1FkOSah19dL7cg7OuBUj9VBJiE2fE ['ly7Hz96gDMc']\n",
      "AIzaSyB-4NIQtecQPbRX7TWKphThkb9_Brh2wL4 ['BCXvnAa-gIk']\n",
      "[[], ['BCXvnAa-gIk']]\n",
      "AIzaSyB-4NIQtecQPbRX7TWKphThkb9_Brh2wL4 ['BCXvnAa-gIk']\n",
      "[[], ['ifKH11DSP18']]\n",
      "AIzaSyB-4NIQtecQPbRX7TWKphThkb9_Brh2wL4 ['ifKH11DSP18']\n",
      "[[], []]\n",
      "[[], []]\n",
      "[[], []]\n",
      "[[], []]\n",
      "[[], []]\n",
      "[[], []]\n",
      "[[], []]\n",
      "[[], []]\n",
      "[[], []]\n",
      "[[], []]\n",
      "[[], []]\n",
      "[['8p9qBAEdxsg', 'PoahCWxhmE4'], ['x3R-Vo6sNOA', 'gMIT7Xa5QmQ', 'Yb60_EEZ9ng']]\n",
      "AIzaSyA4Sd1FkOSah19dL7cg7OuBUj9VBJiE2fE ['8p9qBAEdxsg', 'PoahCWxhmE4']\n",
      "AIzaSyB-4NIQtecQPbRX7TWKphThkb9_Brh2wL4 ['x3R-Vo6sNOA', 'gMIT7Xa5QmQ', 'Yb60_EEZ9ng']\n",
      "[[], []]\n",
      "[[], []]\n",
      "[[], []]\n",
      "[[], []]\n",
      "[[], ['Yb60_EEZ9ng']]\n",
      "AIzaSyB-4NIQtecQPbRX7TWKphThkb9_Brh2wL4 ['Yb60_EEZ9ng']\n",
      "[[], []]\n",
      "[[], []]\n",
      "[[], []]\n",
      "[[], []]\n",
      "[[], []]\n",
      "[[], []]\n",
      "[[], []]\n",
      "[[], []]\n",
      "[[], []]\n",
      "[[], []]\n"
     ]
    }
   ],
   "source": [
    "api_keys = ['AIzaSyA4Sd1FkOSah19dL7cg7OuBUj9VBJiE2fE', 'AIzaSyB-4NIQtecQPbRX7TWKphThkb9_Brh2wL4']\n",
    "#############################\n",
    "# all_video_ids = []\n",
    "# channel_df = get_channel_stats(youtube, channel_ids)\n",
    "# # Initialize dataframes\n",
    "# video_df = pd.DataFrame()\n",
    "# comments_df = pd.DataFrame()\n",
    "# comments_all_data_df = pd.DataFrame()\n",
    "# playlist_df = pd.DataFrame()\n",
    "# playlist_df = get_playlists_info(youtube, channel_ids)\n",
    "# playlist_ids = playlist_df['playlist_id'].tolist()\n",
    "# for playlist_id in playlist_ids:\n",
    "#     print(playlist_id)\n",
    "#     video_ids = get_video_ids(youtube, playlist_id)\n",
    "#     all_video_ids.extend(video_ids)\n",
    "\n",
    "######################################\n",
    "\n",
    "all_video_ids = []\n",
    "comments_all_data_df = pd.DataFrame()\n",
    "playlist_df = pd.DataFrame()\n",
    "channel_df = get_channel_stats(youtube, channel_ids)\n",
    "# Initialize dataframes\n",
    "video_df = pd.DataFrame()\n",
    "comments_df = pd.DataFrame()\n",
    "comments_all_data_df = pd.DataFrame()\n",
    "\n",
    "for c in channel_df['channelName'].unique():\n",
    "    print(\"Getting video information from channel: \" + c)\n",
    "    #playlist_id = channel_df.loc[channel_df['channelName']== c, 'playlistId'].iloc[0]\n",
    "    channel_id = channel_df.loc[channel_df['channelName']== c, 'channel_id'].iloc[0]  # Get the channel_id\n",
    "    playlist_df = get_playlists_info(youtube, channel_ids)\n",
    "    playlist_ids = playlist_df['playlist_id'].tolist()\n",
    "    for playlist_id in playlist_ids:\n",
    "        #print(playlist_id)\n",
    "        video_ids = get_video_ids(youtube, playlist_id)\n",
    "        all_video_ids.extend(video_ids)\n",
    "    \n",
    "    # Split the video_ids list into two parts\n",
    "        split_point = len(video_ids) // 2\n",
    "        video_ids_parts = [video_ids[:split_point], video_ids[split_point:]]\n",
    "        print(video_ids_parts)\n",
    "\n",
    "    \n",
    "\n",
    "# Define a function to fetch data for a given API key and video IDs\n",
    "\n",
    "\n",
    "# Loop through API keys and video ID parts\n",
    "        for api_key, video_ids_part in zip(api_keys, video_ids_parts):\n",
    "            if not video_ids_part:\n",
    "                continue\n",
    "            print(api_key, video_ids_part)\n",
    "            # video_data_part, comments_data_part, comments_combined_part = fetch_data(api_key, video_ids_part, channel_id, playlist_id)\n",
    "            video_data_part  = fetch_data(api_key, video_ids_part, channel_id, playlist_id)\n",
    "            # Append data for the current part to the respective dataframes\n",
    "            video_df = pd.concat([video_df, video_data_part], ignore_index=True)\n",
    "            # comments_df = pd.concat([comments_df, comments_combined_part], ignore_index=True)\n",
    "            # comments_all_data_df = pd.concat([comments_all_data_df, comments_data_part])\n",
    "\n",
    "# playlist_df = get_playlists_info(youtube, channel_ids)\n",
    "\n",
    "# channel_df = get_channel_stats(youtube, channel_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  channel_id     video_id                         playlist_id  \\\n",
      "0   UCupvZG-5ko_eiXAupbDfxWw  sNULLdleZm0  PL6XRrncXkMaU55GiCvv416NR2qBD_xbmf   \n",
      "1   UCupvZG-5ko_eiXAupbDfxWw  EFrJSRbQM8s  PL6XRrncXkMaU55GiCvv416NR2qBD_xbmf   \n",
      "2   UCupvZG-5ko_eiXAupbDfxWw  hB0WgUxhNG0  PL6XRrncXkMaVXAutoJ8D2RDKAz_XufaFm   \n",
      "3   UCupvZG-5ko_eiXAupbDfxWw  sNULLdleZm0  PL6XRrncXkMaVXAutoJ8D2RDKAz_XufaFm   \n",
      "4   UCupvZG-5ko_eiXAupbDfxWw  ly7Hz96gDMc  PL6XRrncXkMaVXAutoJ8D2RDKAz_XufaFm   \n",
      "5   UCupvZG-5ko_eiXAupbDfxWw  6Q7zUBYN3tE  PL6XRrncXkMaVXAutoJ8D2RDKAz_XufaFm   \n",
      "6   UCupvZG-5ko_eiXAupbDfxWw  HHWDbi3DZ0w  PL6XRrncXkMaVXAutoJ8D2RDKAz_XufaFm   \n",
      "7   UCupvZG-5ko_eiXAupbDfxWw  ifKH11DSP18  PL6XRrncXkMaVXAutoJ8D2RDKAz_XufaFm   \n",
      "8   UCupvZG-5ko_eiXAupbDfxWw  pcL0oasrn2Y  PL6XRrncXkMaW8rqNW6ddCsT6SEosWF4W2   \n",
      "9   UCupvZG-5ko_eiXAupbDfxWw  XLOy5ArGYQo  PL6XRrncXkMaW8rqNW6ddCsT6SEosWF4W2   \n",
      "10  UCupvZG-5ko_eiXAupbDfxWw  ly7Hz96gDMc  PL6XRrncXkMaW8rqNW6ddCsT6SEosWF4W2   \n",
      "11  UCupvZG-5ko_eiXAupbDfxWw  sNULLdleZm0  PL6XRrncXkMaWuCgRkfO9o_B2odKQxTBI4   \n",
      "12  UCupvZG-5ko_eiXAupbDfxWw  x3R-Vo6sNOA  PL6XRrncXkMaWuCgRkfO9o_B2odKQxTBI4   \n",
      "13  UCupvZG-5ko_eiXAupbDfxWw  PoahCWxhmE4  PL6XRrncXkMaWuCgRkfO9o_B2odKQxTBI4   \n",
      "14  UCupvZG-5ko_eiXAupbDfxWw  gMIT7Xa5QmQ  PL6XRrncXkMaWuCgRkfO9o_B2odKQxTBI4   \n",
      "15  UCupvZG-5ko_eiXAupbDfxWw  up5t8Akof18  PL6XRrncXkMaWuCgRkfO9o_B2odKQxTBI4   \n",
      "16  UCupvZG-5ko_eiXAupbDfxWw  8p9qBAEdxsg  PL6XRrncXkMaWuCgRkfO9o_B2odKQxTBI4   \n",
      "17  UCupvZG-5ko_eiXAupbDfxWw  pYbeeh7syNk  PL6XRrncXkMaWuCgRkfO9o_B2odKQxTBI4   \n",
      "18  UCupvZG-5ko_eiXAupbDfxWw  DrtEEYEZMNU  PL6XRrncXkMaWuCgRkfO9o_B2odKQxTBI4   \n",
      "19  UCupvZG-5ko_eiXAupbDfxWw  Jnd6-N0sdT4  PL6XRrncXkMaWuCgRkfO9o_B2odKQxTBI4   \n",
      "\n",
      "   channelTitle                                              title  \\\n",
      "0           CNN  Huge concern: Military analyst on new fears ...   \n",
      "1           CNN  CNN presses IDF spokesperson on the ratio of c...   \n",
      "2           CNN  COP28 president defends his commitment to science   \n",
      "3           CNN  Huge concern: Military analyst on new fears ...   \n",
      "4           CNN  American WSJ reporter Evan Gershkovich held fo...   \n",
      "5           CNN  Another reporter vanished in China. Here's wha...   \n",
      "6           CNN       Anderson reacts to Oxfords word of the year   \n",
      "7           CNN  These Ukrainians are entering a 'death trap' t...   \n",
      "8           CNN  Tapper calls Biden admission about 2024 presid...   \n",
      "9           CNN  Biden to donors: Not sure Id be running in ...   \n",
      "10          CNN  American WSJ reporter Evan Gershkovich held fo...   \n",
      "11          CNN  Huge concern: Military analyst on new fears ...   \n",
      "12          CNN  Tim Alberta: Why evangelicals feel Trump is li...   \n",
      "13          CNN  This is going to be insane: Van Jones on a p...   \n",
      "14          CNN  See George Santos' surprise next move after be...   \n",
      "15          CNN  'This wasn't some impromptu ad-lib': Maggie Ha...   \n",
      "16          CNN  George Santos sells videos on Cameo after bein...   \n",
      "17          CNN  Fetterman trolls indicted Senate colleague wit...   \n",
      "18          CNN  Haley wants to reform Social Security and Medi...   \n",
      "19          CNN  Hear audio of Trump talking about a pre-2024 W...   \n",
      "\n",
      "                                          description  \\\n",
      "0   New US strikes on Iranian-backed militants, fa...   \n",
      "1   CNN's Erin Burnett asks Lt. Col. Jonathan Conr...   \n",
      "2   The President of this year's COP28 climate sum...   \n",
      "3   New US strikes on Iranian-backed militants, fa...   \n",
      "4   CNN's Jake Tapper speaks to the parents of Wal...   \n",
      "5   Veteran Hong Kong-based journalist Minnie Chan...   \n",
      "6   Language experts at dictionary publisher Oxfor...   \n",
      "7   CNN's Anna Coren follows a unit of Ukrainians ...   \n",
      "8   President Joe Biden told Democratic donors he ...   \n",
      "9   President Joe Biden told Democratic donors Tue...   \n",
      "10  CNN's Jake Tapper speaks to the parents of Wal...   \n",
      "11  New US strikes on Iranian-backed militants, fa...   \n",
      "12  The Atlantic reporter Tim Alberta talks to CNN...   \n",
      "13  CNNs Kristen Holmes looks at Donald Trumps d...   \n",
      "14  Former New York congressman George Santos is t...   \n",
      "15  New York Times political reporter Maggie Haber...   \n",
      "16  Former Republican Congressman George Santos ha...   \n",
      "17  Sen. John Fetterman (D-PA) commissioned expell...   \n",
      "18  Republican presidential candidate Nikki Haley ...   \n",
      "19  Chief Washington correspondent for ABC News Jo...   \n",
      "\n",
      "                                                 tags           publishedAt  \\\n",
      "0                                                None  2023-12-04T22:15:12Z   \n",
      "1   [Erin Burnett, Israel, Gaza, IDF, Lt. Col. Jon...  2023-12-05T17:05:12Z   \n",
      "2                                                None  2023-12-04T21:09:37Z   \n",
      "3                                                None  2023-12-04T22:15:12Z   \n",
      "4   [Evan Gershkovich, Russia, Jake Tapper, The Le...  2023-12-04T23:45:01Z   \n",
      "5   [Minnie Chan, Will Ripley, Erin Burnett, China...  2023-12-05T04:00:03Z   \n",
      "6   [CNN, News, Top News, Oxford, dictionary, word...  2023-12-05T05:27:39Z   \n",
      "7   [#Ukraine, #Deathtrap, #Russia, #war, #Whitean...  2023-12-05T23:39:20Z   \n",
      "8                                                None  2023-12-05T23:36:53Z   \n",
      "9                                                None  2023-12-05T22:45:46Z   \n",
      "10  [Evan Gershkovich, Russia, Jake Tapper, The Le...  2023-12-04T23:45:01Z   \n",
      "11                                               None  2023-12-04T22:15:12Z   \n",
      "12                                               None  2023-12-04T23:15:55Z   \n",
      "13                                               None  2023-12-05T00:13:12Z   \n",
      "14                                               None  2023-12-05T01:27:24Z   \n",
      "15  [Maggie Haberman, New York Times, Kaitlan Coll...  2023-12-05T03:30:24Z   \n",
      "16  [CNN, News, US, Politics, George Santos, John ...  2023-12-05T03:26:42Z   \n",
      "17  [latest news, Happening Now, CNN, Poppy Harlow...  2023-12-05T15:30:59Z   \n",
      "18  [Nikki Haley, Alyssa Farah Griffin, 2024 presi...  2023-12-05T16:26:21Z   \n",
      "19  [latest news, Happening now, CNN, Dana Bash, I...  2023-12-05T18:46:17Z   \n",
      "\n",
      "   defaultAudioLanguage viewCount likeCount favouriteCount commentCount  \\\n",
      "0                    en    130207      1160           None         1028   \n",
      "1                    en     38021       519           None         1785   \n",
      "2                    en     13674       309           None           51   \n",
      "3                    en    130207      1160           None         1028   \n",
      "4                    en      9858       142           None          204   \n",
      "5                    en     58612      1026           None          688   \n",
      "6                    en     50525      1215           None          110   \n",
      "7                    en      2718       133           None          138   \n",
      "8                    en      5706       501           None          114   \n",
      "9                    en     25256       739           None          760   \n",
      "10                   en      9858       142           None          204   \n",
      "11                   en    130207      1160           None         1028   \n",
      "12                   en     71172      1050           None          934   \n",
      "13                   en    138080      1837           None         1998   \n",
      "14                   en    300209      2621           None         1451   \n",
      "15                   en    221250      3079           None         1170   \n",
      "16                   en     33643      1096           None          175   \n",
      "17                   en     99109      1475           None          711   \n",
      "18                   en     74683      1381           None          950   \n",
      "19                   en    222954      4072           None         1436   \n",
      "\n",
      "    duration definition caption  \n",
      "0     PT5M8S         hd    true  \n",
      "1    PT5M28S         hd    true  \n",
      "2      PT47S         hd   false  \n",
      "3     PT5M8S         hd    true  \n",
      "4    PT5M38S         hd    true  \n",
      "5    PT3M46S         hd    true  \n",
      "6      PT47S         hd   false  \n",
      "7    PT4M27S         sd    true  \n",
      "8      PT59S         hd   false  \n",
      "9    PT8M47S         hd    true  \n",
      "10   PT5M38S         hd    true  \n",
      "11    PT5M8S         hd    true  \n",
      "12   PT7M23S         hd    true  \n",
      "13  PT10M59S         hd    true  \n",
      "14   PT2M56S         hd    true  \n",
      "15   PT4M24S         hd    true  \n",
      "16    PT1M1S         hd   false  \n",
      "17   PT4M14S         hd    true  \n",
      "18   PT8M39S         hd    true  \n",
      "19   PT6M20S         hd    true  \n"
     ]
    }
   ],
   "source": [
    "print(video_df.head(20))\n",
    "#video_df.to_csv('video_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cee815cb-46e2-40d0-85a0-a56db0dd42ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with video statistics and comments from all channels\n",
    "\n",
    "# video_df = pd.DataFrame()\n",
    "# comments_df = pd.DataFrame()\n",
    "# comments_all_data_df = pd.DataFrame()\n",
    "\n",
    "# for c in channel_df['channelName'].unique():\n",
    "#     print(\"Getting video information from channel: \" + c)\n",
    "#     playlist_id = channel_df.loc[channel_df['channelName']== c, 'playlistId'].iloc[0]\n",
    "#     channel_id = channel_df.loc[channel_df['channelName']== c, 'channel_id'].iloc[0]  # Get the channel_id\n",
    "#     video_ids = get_video_ids(youtube, playlist_id)\n",
    "    \n",
    "#     # get video data\n",
    "#     video_data = get_video_details(youtube, video_ids, channel_id)  # Pass channel_id\n",
    "#     # get comment data\n",
    "#     #comments_data_df, comments_combined_df = get_comments(youtube, video_ids)\n",
    "\n",
    "#     # append video data together and comment data toghether\n",
    "#     video_df = video_df.append(video_data, ignore_index=True)\n",
    "#     comments_df = comments_df.append(comments_combined_df, ignore_index=True)\n",
    "#     comments_all_data_df = comments_all_data_df.append(comments_data_df)\n",
    "\n",
    "# playlist_df = get_playlists_info(youtube, channel_ids)\n",
    "\n",
    "# channel_df = get_channel_stats(youtube, channel_ids)\n",
    "\n",
    "# #captions_df = get_captions(youtube, video_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25755eef",
   "metadata": {},
   "source": [
    "## Importing the data into Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dceddedd-f561-44c5-be35-5add6ad0ed60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.connector\n",
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Snowflake connection parameters\n",
    "snowflake_user = 'FURNITUREWALAABBAS'\n",
    "snowflake_password = 'Abba$123'\n",
    "snowflake_account = 'jrnvcvi-sw72415'\n",
    "snowflake_database = 'YOUTUBE_LLM'\n",
    "snowflake_schema = 'PUBLIC'\n",
    "#snowflake_warehouse = 'your_warehouse'\n",
    "\n",
    "# Create a Snowflake connection\n",
    "conn = snowflake.connector.connect(\n",
    "    user=snowflake_user,\n",
    "    password=snowflake_password,\n",
    "    account=snowflake_account,\n",
    "    #warehouse=snowflake_warehouse,\n",
    "    database=snowflake_database,\n",
    "    schema=snowflake_schema\n",
    ")\n",
    "\n",
    "# Create a cursor object\n",
    "cur = conn.cursor()\n",
    "\n",
    "# cur.execute(\"CREATE OR REPLACE TABLE YOUTUBE_LLM.PUBLIC.VIDEOS ( \\\n",
    "#     channel_id STRING, \\\n",
    "#     video_id STRING,\\\n",
    "#     channelTitle STRING,\\\n",
    "#     title STRING,\\\n",
    "#     description STRING,\\\n",
    "#     tags VARIANT, \\\n",
    "#     publishedAt TIMESTAMP_NTZ,\\\n",
    "#     defaultAudioLanguage STRING, \\\n",
    "#     viewCount FLOAT,\\\n",
    "#     likeCount FLOAT,\\\n",
    "#     favouriteCount FLOAT,\\\n",
    "#     commentCount FLOAT,\\\n",
    "#     duration STRING, \\\n",
    "#     definition STRING,\\\n",
    "#     caption BOOLEAN,\\\n",
    "#     pushblishDayName STRING,\\\n",
    "#     durationSecs FLOAT,\\\n",
    "#     tagsCount INTEGER,\\\n",
    "#     likeRatio FLOAT,\\\n",
    "#     commentRatio FLOAT,\\\n",
    "#     titleLength INTEGER)\" ) \n",
    "\n",
    "# # LOAD TABLE VIDEOS\n",
    " \n",
    "# write_pandas(conn, video_df, 'VIDEOS', quote_identifiers= False)\n",
    "\n",
    "# cur.execute(\"CREATE OR REPLACE TABLE YOUTUBE_LLM.PUBLIC.COMMENTS ( \\\n",
    "#     VIDEO_ID VARCHAR(1000000) ,  \\\n",
    "#     COMMENTS VARCHAR(16777216),  \\\n",
    "#     LIKECOUNT NUMBER(38, 0), \\\n",
    "#     AUTHORDISPLAYNAME VARCHAR(500), \\\n",
    "#     AUTHORPROFILEIMAGEURL VARCHAR(10000),\\\n",
    "#     AUTHORCHANNELURL VARCHAR(10000),\\\n",
    "#     AUTHORCHANNELID VARCHAR(10000),\\\n",
    "#     CHANNELID VARCHAR(10000),\\\n",
    "#     CANRATE BOOLEAN,\\\n",
    "#     VIEWERRATING VARCHAR(10000),\\\n",
    "#     PUBLISHEDAT TIMESTAMP_NTZ(9))\")\n",
    "\n",
    "\n",
    "# write_pandas(conn, comments_all_data_df, 'COMMENTS', quote_identifiers= False)\n",
    "\n",
    "# CREATING TABLE COMMENTS_COMBINED\n",
    "\n",
    "# cur.execute(\"create or replace TABLE YOUTUBE_LLM.PUBLIC.COMMENTS_COMBINED ( \\\n",
    "# \tVIDEO_ID VARCHAR(10000000),\\\n",
    "# \tCOMMENTS VARIANT);\")\n",
    "\n",
    "# write_pandas(conn, comments_df, 'COMMENTS_COMBINED', quote_identifiers= False)\n",
    "\n",
    "# cur.execute(\"create or replace TABLE YOUTUBE_LLM.PUBLIC.CHANNELS ( \\\n",
    "# \tCHANNELNAME VARCHAR(16777216),\\\n",
    "# \tCHANNEL_ID VARCHAR(16777216),\\\n",
    "# \tSUBSCRIBERS NUMBER(38,0),\\\n",
    "# \tVIEWS NUMBER(38,0),\\\n",
    "# \tTOTALVIDEOS NUMBER(38,0),\\\n",
    "# \tPLAYLISTID VARCHAR(16777216) )\")\n",
    "\n",
    "\n",
    "# write_pandas(conn, channel_df, 'CHANNELS', quote_identifiers= False)\n",
    "\n",
    "# CREATING TABLE PLAYLIST\n",
    "\n",
    "cur.execute(\"create or replace TABLE YOUTUBE_LLM.PUBLIC.PLAYLIST (\\\n",
    "\tPLAYLIST_ID VARCHAR(10000000),\\\n",
    "\tTITLE VARCHAR(10000000),\\\n",
    "\tDESCRIPTION VARCHAR(10000000),\\\n",
    "\tPUBLISHEDAT TIMESTAMP_NTZ(9),\\\n",
    "\tCHANNELID VARCHAR(10000000),\\\n",
    "\tCHANNELTITLE VARCHAR(10000000),\\\n",
    "\tDEFAULTLANGUAGE VARCHAR(10000000),\\\n",
    "\tTHUMBNAILURL VARCHAR(10000000));\")\n",
    "\n",
    "\n",
    "write_pandas(conn, playlist_df, 'PLAYLIST', quote_identifiers= False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the table name\n",
    "# table_name = 'CHANNELS'\n",
    "\n",
    "# Create an internal stage (temporary storage for data)\n",
    "# stage_name = 'STAGING'\n",
    "# cur.execute(f'CREATE OR REPLACE STAGE {stage_name}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Upload the DataFrame to Snowflake stage\n",
    "# csv_filename = 'video_data1.csv'\n",
    "# video_df1.to_csv(csv_filename, index=False)\n",
    "#cur.execute(r\"PUT file:///C:\\Users\\furni\\youtube-api-analysis\\video_data1.csv @STAGING_TABLES AUTO_COMPRESS=TRUE\")\n",
    "\n",
    "# Copy data from the stage into a Snowflake table\n",
    "\n",
    "    \n",
    "\n",
    "# # Copy data from the stage into the Snowflake table\n",
    "# csv_filepath = r'C:\\\\Users\\\\furni\\\\youtube-api-analysis\\\\video_data1.csv'\n",
    "# copy_query = f'''COPY INTO VIDEOS FROM 'file://{csv_filepath}' FILE_FORMAT = (TYPE = CSV SKIP_HEADER = 1)'''\n",
    "# cur.execute(copy_query)\n",
    "\n",
    "# Commit the changes\n",
    "conn.commit()\n",
    "\n",
    "# Close the cursor and connection\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "#print(f'DataFrame has been successfully uploaded as table: {table_name}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
